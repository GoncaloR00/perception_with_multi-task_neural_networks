{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f19105e-f316-451c-8844-c378d0825e73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gribeiro/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch_tensorrt\n",
    "\n",
    "# print(tensorrt.IBuilderConfig.max_workspace_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48919651-337d-4d05-b7a8-d6e1a63b791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = torch.jit.load(\"./models/yolov5s.torchscript\").to(\"cuda\")\n",
    "# load_model = torch.jit.load(\"./models/yolopv2.pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96a806fe-788e-4d00-a6f9-d033364a4b32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gribeiro/.local/lib/python3.8/site-packages/torch/jit/_trace.py:782: UserWarning: The input to trace is already a ScriptModule, tracing it is a no-op. Returning the object as is.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "traced_model = torch.jit.trace(load_model, [torch.randn((1, 3, 384, 640)).to(\"cuda\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff041ae3-5cac-4239-8f7f-8404d910533e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.device('cuda')\n",
    "load_model = load_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a412af37-765e-48de-a2cf-d7323d758398",
   "metadata": {},
   "outputs": [],
   "source": [
    "trt_model = torch_tensorrt.compile(\n",
    "    load_model,\n",
    "    inputs = [torch_tensorrt.Input((1, 3, 384, 640), dtype=torch.float32)],\n",
    "    enabled_precisions = {torch.float32},\n",
    "    truncate_long_and_double = True,\n",
    "    device = torch_tensorrt.Device(\"cuda:0\"),\n",
    "    # workspace_size=4194304\n",
    "    workspace_size=300000\n",
    ")\n",
    "# torch_tensorrt.dtype.half"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2500362a-cae8-448b-ad09-d4f1be99bce9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Dilation not used in Max pooling converter\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Int64 to Int32\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Int64 to Int32\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Int64 to Int32\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Int64 to Int32\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Int64 to Int32\n",
      "WARNING: [Torch-TensorRT] - Truncating weight (constant in the graph) from Int64 to Int32\n"
     ]
    }
   ],
   "source": [
    "trt_model_half = torch_tensorrt.compile(\n",
    "    load_model,\n",
    "    inputs = [torch.randn((1, 3, 384, 640), dtype=torch.float16)],\n",
    "    enabled_precisions = {torch.float16},\n",
    "    truncate_long_and_double = True,\n",
    "    device = torch_tensorrt.Device(\"cuda:0\"),\n",
    "    workspace_size=4194304\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfc2d534-36c5-4191-aefe-125ecdfede02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import torch.backends.cudnn as cudnn\n",
    "cudnn.benchmark = True\n",
    "\n",
    "def benchmark(model, device=\"cuda:0\", input_shape=(1, 3, 384, 640), dtype='fp32', nwarmup=50, nruns=100):\n",
    "    if dtype == 'fp16':\n",
    "        input_data = torch.randn(input_shape, dtype = torch.half)\n",
    "    else:\n",
    "        input_data = torch.randn(input_shape)\n",
    "    # input_data.half()\n",
    "    input_data = input_data.to(device)\n",
    "        \n",
    "    print(\"Warm up ...\")\n",
    "    with torch.no_grad():\n",
    "        for _ in range(nwarmup):\n",
    "            features = model(input_data)\n",
    "    torch.cuda.synchronize()\n",
    "    print(\"Start timing ...\")\n",
    "    timings = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, nruns+1):\n",
    "            start_time = time.time()\n",
    "            features = model(input_data)\n",
    "            torch.cuda.synchronize()\n",
    "            end_time = time.time()\n",
    "            timings.append(end_time - start_time)\n",
    "            if i%10==0:\n",
    "                print('Iteration %d/%d, ave batch time %.2f ms'%(i, nruns, np.mean(timings)*1000))\n",
    "\n",
    "    print(\"Input shape:\", input_data.size())\n",
    "    # print(\"Output features size:\", features.size())\n",
    "    print('Average batch time: %.2f ms'%(np.mean(timings)*1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f1b19a4-8bb3-4ac7-8f96-461bd889ac1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 10/100, ave batch time 10.96 ms\n",
      "Iteration 20/100, ave batch time 10.30 ms\n",
      "Iteration 30/100, ave batch time 10.15 ms\n",
      "Iteration 40/100, ave batch time 10.03 ms\n",
      "Iteration 50/100, ave batch time 10.10 ms\n",
      "Iteration 60/100, ave batch time 10.09 ms\n",
      "Iteration 70/100, ave batch time 10.05 ms\n",
      "Iteration 80/100, ave batch time 10.04 ms\n",
      "Iteration 90/100, ave batch time 10.01 ms\n",
      "Iteration 100/100, ave batch time 9.97 ms\n",
      "Input shape: torch.Size([1, 3, 384, 640])\n",
      "Average batch time: 9.97 ms\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "model_half = copy.deepcopy(load_model).half()\n",
    "benchmark(model_half, dtype='fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1860e05f-f0aa-45fc-acca-72ffa73c4c34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 10/100, ave batch time 15.64 ms\n",
      "Iteration 20/100, ave batch time 15.95 ms\n",
      "Iteration 30/100, ave batch time 15.88 ms\n",
      "Iteration 40/100, ave batch time 15.84 ms\n",
      "Iteration 50/100, ave batch time 15.77 ms\n",
      "Iteration 60/100, ave batch time 15.79 ms\n",
      "Iteration 70/100, ave batch time 15.78 ms\n",
      "Iteration 80/100, ave batch time 15.79 ms\n",
      "Iteration 90/100, ave batch time 15.79 ms\n",
      "Iteration 100/100, ave batch time 15.78 ms\n",
      "Input shape: torch.Size([1, 3, 384, 640])\n",
      "Average batch time: 15.78 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(load_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9749a346-fe58-4a0f-a354-5dd05007502d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 10/100, ave batch time 1.14 ms\n",
      "Iteration 20/100, ave batch time 1.12 ms\n",
      "Iteration 30/100, ave batch time 1.11 ms\n",
      "Iteration 40/100, ave batch time 1.11 ms\n",
      "Iteration 50/100, ave batch time 1.11 ms\n",
      "Iteration 60/100, ave batch time 1.10 ms\n",
      "Iteration 70/100, ave batch time 1.10 ms\n",
      "Iteration 80/100, ave batch time 1.10 ms\n",
      "Iteration 90/100, ave batch time 1.10 ms\n",
      "Iteration 100/100, ave batch time 1.10 ms\n",
      "Input shape: torch.Size([1, 3, 384, 640])\n",
      "Average batch time: 1.10 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(trt_model_half, dtype='fp16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "307ebb1b-617a-4e73-8eaf-97bfe55c12ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warm up ...\n",
      "Start timing ...\n",
      "Iteration 10/100, ave batch time 14.61 ms\n",
      "Iteration 20/100, ave batch time 14.90 ms\n",
      "Iteration 30/100, ave batch time 14.90 ms\n",
      "Iteration 40/100, ave batch time 14.92 ms\n",
      "Iteration 50/100, ave batch time 14.86 ms\n",
      "Iteration 60/100, ave batch time 14.90 ms\n",
      "Iteration 70/100, ave batch time 14.86 ms\n",
      "Iteration 80/100, ave batch time 14.86 ms\n",
      "Iteration 90/100, ave batch time 14.88 ms\n",
      "Iteration 100/100, ave batch time 14.91 ms\n",
      "Input shape: torch.Size([1, 3, 384, 640])\n",
      "Average batch time: 14.91 ms\n"
     ]
    }
   ],
   "source": [
    "benchmark(trt_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c8bbfc2-c7e4-4709-9fd0-4f4335b93cf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Input 0 of engine __torch___models_yolo_DetectionModel_trt_engine_0x51092d60 was found to be on cpu but should be on cuda:0. This tensor is being moved by the runtime but for performance considerations, ensure your inputs are all on GPU and open an issue here (https://github.com/pytorch/TensorRT/issues) if this warning persists.\n",
      "WARNING: [Torch-TensorRT] - Input 0 of engine __torch___models_yolo_DetectionModel_trt_engine_0x51092f70 was found to be on cpu but should be on cuda:0. This tensor is being moved by the runtime but for performance considerations, ensure your inputs are all on GPU and open an issue here (https://github.com/pytorch/TensorRT/issues) if this warning persists.\n",
      "WARNING: [Torch-TensorRT] - Input 1 of engine __torch___models_yolo_DetectionModel_trt_engine_0x51092f70 was found to be on cpu but should be on cuda:0. This tensor is being moved by the runtime but for performance considerations, ensure your inputs are all on GPU and open an issue here (https://github.com/pytorch/TensorRT/issues) if this warning persists.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: [Error thrown at core/runtime/execute_engine.cpp:136] Expected inputs[i].dtype() == expected_type to be true but got false\nExpected input tensors to have type Half, found type float\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrt_model_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m384\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m640\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhalf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: [Error thrown at core/runtime/execute_engine.cpp:136] Expected inputs[i].dtype() == expected_type to be true but got false\nExpected input tensors to have type Half, found type float\n\n"
     ]
    }
   ],
   "source": [
    "trt_model_half(torch.randn((1, 3, 384, 640), dtype = torch.half))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a617d81-4ccf-4277-833b-6d2acf105c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "teste = torch.randn((1, 3, 384, 640), dtype=torch.float16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1378e8c7-30df-40ed-952d-ea5a4f03477d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.1689,  0.0921,  1.7334,  ...,  1.1240, -0.9038, -1.4434],\n",
       "          [-0.0855,  0.6484,  0.1616,  ..., -0.5337, -1.3232, -0.3740],\n",
       "          [-0.6460,  0.3330, -0.0072,  ...,  0.5352,  1.4834,  1.0596],\n",
       "          ...,\n",
       "          [-0.9326,  0.8481, -0.3757,  ...,  1.0459,  0.3838, -0.6147],\n",
       "          [ 0.8218, -1.5693, -0.7974,  ...,  1.5107, -0.9663, -0.0483],\n",
       "          [ 1.7832, -1.4648, -0.8350,  ...,  0.5840, -0.8550,  0.7383]],\n",
       "\n",
       "         [[-1.3564, -0.6748, -0.3154,  ...,  0.4971,  0.0774,  1.5732],\n",
       "          [-0.0685,  0.3486,  0.4094,  ..., -0.7871,  0.3789, -1.7520],\n",
       "          [-0.0982, -0.6846,  1.2363,  ..., -1.0967,  0.5737, -1.5830],\n",
       "          ...,\n",
       "          [ 0.5210, -0.8164, -0.7412,  ...,  1.1729,  0.7275, -1.8613],\n",
       "          [ 0.5464, -0.0115,  0.2756,  ...,  0.9985,  1.8457,  1.5156],\n",
       "          [-0.5640, -0.8818,  0.3110,  ..., -0.3245,  0.1918,  0.9941]],\n",
       "\n",
       "         [[-0.1318, -1.9062, -0.3010,  ...,  0.6899, -0.2499,  0.7744],\n",
       "          [ 0.2620,  0.5225,  0.1969,  ..., -0.3706, -0.7178,  1.8301],\n",
       "          [-0.8584,  0.0621,  0.1249,  ..., -1.4492,  1.0615,  0.3723],\n",
       "          ...,\n",
       "          [ 0.6289,  0.6260, -1.0703,  ...,  0.8550, -0.5918, -0.2318],\n",
       "          [-0.4045, -0.3884, -1.0898,  ...,  0.5747,  0.6284,  0.0904],\n",
       "          [ 2.1602, -1.5938, -0.4290,  ..., -0.0635, -2.2402,  0.2227]]]],\n",
       "       dtype=torch.float16)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teste.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ad7582ef-abda-45ac-820a-9312b6570454",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch_tensorrt._Input.Input at 0x7f12c4389790>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensorrt.Input((1, 3, 384, 640), dtype=torch_tensorrt.dtype.half)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3ff45286-8d92-4399-bc36-89127efaefea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: [Torch-TensorRT] - Input 0 of engine __torch___models_yolo_DetectionModel_trt_engine_0xe8860190 was found to be on cpu but should be on cuda:0. This tensor is being moved by the runtime but for performance considerations, ensure your inputs are all on GPU and open an issue here (https://github.com/pytorch/TensorRT/issues) if this warning persists.\n",
      "WARNING: [Torch-TensorRT] - Input 0 of engine __torch___models_yolo_DetectionModel_trt_engine_0xe88603a0 was found to be on cpu but should be on cuda:0. This tensor is being moved by the runtime but for performance considerations, ensure your inputs are all on GPU and open an issue here (https://github.com/pytorch/TensorRT/issues) if this warning persists.\n",
      "WARNING: [Torch-TensorRT] - Input 1 of engine __torch___models_yolo_DetectionModel_trt_engine_0xe88603a0 was found to be on cpu but should be on cuda:0. This tensor is being moved by the runtime but for performance considerations, ensure your inputs are all on GPU and open an issue here (https://github.com/pytorch/TensorRT/issues) if this warning persists.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: [Error thrown at core/runtime/execute_engine.cpp:136] Expected inputs[i].dtype() == expected_type to be true but got false\nExpected input tensors to have type Half, found type float\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrt_model_half\u001b[49m\u001b[43m(\u001b[49m\u001b[43mteste\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\nRuntimeError: [Error thrown at core/runtime/execute_engine.cpp:136] Expected inputs[i].dtype() == expected_type to be true but got false\nExpected input tensors to have type Half, found type float\n\n"
     ]
    }
   ],
   "source": [
    "trt_model_half(teste)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3357d0ce-467e-47ad-9c6c-825ea40d2c7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torch_tensorrt' from '/home/gribeiro/.local/lib/python3.8/site-packages/torch_tensorrt/__init__.py'>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch_tensorrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d2abe3d-1446-44bb-8210-641b09c7cdfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.13.1+cu117'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b314e4b-bbba-4a0b-91d7-66187fc39d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.jit.save(trt_model_half, 'yolov5s_trtfp32.trt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
